{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9Diqdoz4XkHghDXP4O9OS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sravanbunny2002/Sentimental-Analysis-on-IMDB-Movie-Reviews/blob/main/Sentimental_Analysis_on_IMDB_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries"
      ],
      "metadata": {
        "id": "YXbH9yJgX5Gq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ek3baRWKxnx"
      },
      "outputs": [],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Nltk  and download the stopwords"
      ],
      "metadata": {
        "id": "GatL-_20X_GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "metadata": {
        "id": "IGBF4w0wTei7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zp0XlQYVYPYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the training data\n",
        "from google.colab import files\n",
        "uploaded=files.upload()\n"
      ],
      "metadata": {
        "id": "nHfV9I2kK-qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset and setting it to the Dataframe"
      ],
      "metadata": {
        "id": "MrpFUPraYSbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['Dataset_Movie_Reviews.csv']))"
      ],
      "metadata": {
        "id": "6GrueNvGRoCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2)"
      ],
      "metadata": {
        "id": "LS7tKAE_K-yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(10)"
      ],
      "metadata": {
        "id": "imCaeNEpK-1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "g3JHGtMnK-3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "_aKe3rBFK-7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the training dataset\n"
      ],
      "metadata": {
        "id": "pmV6ADbrYeQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset  \n",
        "#train dataset\n",
        "train_reviews=df2.review[:1000]\n",
        "train_sentiments=df2.sentiment[:1000]\n",
        "#test dataset\n",
        "test_reviews=df2.review[1000:]\n",
        "test_sentiments=df2.sentiment[1000:]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)\n"
      ],
      "metadata": {
        "id": "U5uOt7heK-9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization of text"
      ],
      "metadata": {
        "id": "CIiTiE0iYlu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "VdFyrOGgK_A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df2['review']=df2['review'].apply(denoise_text)"
      ],
      "metadata": {
        "id": "yy1HsWD-K_D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df2['review']=df2['review'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "8XlhTzIyUjLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df2['review']=df2['review'].apply(simple_stemmer)"
      ],
      "metadata": {
        "id": "v_THIsc7UjN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "df2['review']=df2['review'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "dsZeGeLDUjQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalized train reviews\n",
        "norm_train_reviews=df2.review[:1000]\n",
        "norm_train_reviews[0]\n",
        "#convert dataframe to string\n",
        "#norm_train_string=norm_train_reviews.to_string()\n",
        "#Spelling correction using Textblob\n",
        "#norm_train_spelling=TextBlob(norm_train_string)\n",
        "#norm_train_spelling.correct()\n",
        "#Tokenization using Textblob\n",
        "#norm_train_words=norm_train_spelling.words\n",
        "#norm_train_words"
      ],
      "metadata": {
        "id": "AiRFKnM3UjTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized test reviews\n",
        "norm_test_reviews=df2.review[1000:]\n",
        "norm_test_reviews[1505]\n",
        "##convert dataframe to string\n",
        "#norm_test_string=norm_test_reviews.to_string()\n",
        "#spelling correction using Textblob\n",
        "#norm_test_spelling=TextBlob(norm_test_string)\n",
        "#print(norm_test_spelling.correct())\n",
        "#Tokenization using Textblob\n",
        "#norm_test_words=norm_test_spelling.words\n",
        "#norm_test_words"
      ],
      "metadata": {
        "id": "t5aLIF1sVF3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency-Inverse Document Frequency model (TFIDF)\n",
        "\n",
        "It is used to convert text documents to matrix of tfidf features."
      ],
      "metadata": {
        "id": "nHdaBF3gZNEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ],
      "metadata": {
        "id": "5zIMbyB_WWMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bags of words model\n",
        "\n",
        "It is used to convert text documents to numerical vectors or bag of words."
      ],
      "metadata": {
        "id": "EHCHUmhwZUS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)\n",
        "#vocab=cv.get_feature_names()-toget feature names"
      ],
      "metadata": {
        "id": "GfjjHWanVU-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labeling the sentiment text"
      ],
      "metadata": {
        "id": "zXGc13dCZZgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#labeling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "#transformed sentiment data\n",
        "sentiment_data=lb.fit_transform(df2['sentiment'])\n",
        "print(sentiment_data.shape)"
      ],
      "metadata": {
        "id": "0BadSAYhVF6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting the sentiment data\n",
        "train_sentiments=sentiment_data[:1000]\n",
        "test_sentiments=sentiment_data[1000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)\n"
      ],
      "metadata": {
        "id": "bUy6_gQwVF92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling the dataset\n",
        "\n",
        " logistic regression model for both bag of words and tfidf features"
      ],
      "metadata": {
        "id": "Tr7k5fWqZlrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
        "print(lr_bow)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
        "print(lr_tfidf)"
      ],
      "metadata": {
        "id": "OLoxvWBwWC7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance of the model"
      ],
      "metadata": {
        "id": "fYEHa7wCZsow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "##Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)\n"
      ],
      "metadata": {
        "id": "WfYHjeGPVGAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy of the LRM\n"
      ],
      "metadata": {
        "id": "z2rROrL9ZrIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy score for bag of words\n",
        "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ],
      "metadata": {
        "id": "v_HTi6_ZVGDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification report for bag of words \n",
        "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "#Classification report for tfidf features\n",
        "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(lr_tfidf_report)"
      ],
      "metadata": {
        "id": "QH9f9TJUVGGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ],
      "metadata": {
        "id": "tOEmSXkyVGI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word cloud for positive review words\n",
        "plt.figure(figsize=(10,10))\n",
        "positive_text=norm_train_reviews[1]\n",
        "WC=WordCloud(width=1000,height=500,max_words=50,min_font_size=5)\n",
        "positive_words=WC.generate(positive_text)\n",
        "plt.imshow(positive_words,interpolation='bilinear')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "029oOt7oW56o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word cloud for negative review words\n",
        "plt.figure(figsize=(10,10))\n",
        "negative_text=norm_train_reviews[8]\n",
        "WC=WordCloud(width=1000,height=500,max_words=50,min_font_size=5)\n",
        "negative_words=WC.generate(negative_text)\n",
        "plt.imshow(negative_words,interpolation='bilinear')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "thv7ozuSW59F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yxy4azs7W6AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbwAL18AW6CG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}